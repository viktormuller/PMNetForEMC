{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73542dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def find_missing_numbers(input_csv, output_csv):\n",
    "    # Read the integers from the input CSV\n",
    "    with open(input_csv, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        #next(reader, None)  # Skip header\n",
    "        numbers = [int(row[0]) for row in reader]\n",
    "\n",
    "    # Find the missing numbers\n",
    "    full_set = set(range(1, 19016))\n",
    "    missing_numbers = sorted(list(full_set - set(numbers)))\n",
    "\n",
    "    # Write the missing numbers to the output CSV\n",
    "    with open(output_csv, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        #writer.writerow(['Missing Numbers'])  # Header\n",
    "        for number in missing_numbers:\n",
    "            writer.writerow([number])\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'data/USC/train.csv'  # Replace with your input CSV file path\n",
    "output_csv = 'data/USC/test.csv'  # The output file will be created with this name\n",
    "find_missing_numbers(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419ed2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vikto\\python\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\vikto\\python\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vikto\\python\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vikto\\python\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vikto\\python\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vikto\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\vikto\\python\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vikto\\python\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vikto\\python\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vikto\\python\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vikto\\python\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\vikto\\python\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vikto\\python\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\vikto\\python\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vikto\\python\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vikto\\python\\lib\\site-packages (from torch==2.1.2->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vikto\\python\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikto\\python\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikto\\python\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikto\\python\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vikto\\python\\lib\\site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vikto\\python\\lib\\site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\vikto\\python\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (1.11.4)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (2023.12.9)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (22.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in c:\\users\\vikto\\python\\lib\\site-packages (from scikit-image) (0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.2-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.47.0-cp311-cp311-win_amd64.whl.metadata (160 kB)\n",
      "     ---------------------------------------- 0.0/160.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 160.4/160.4 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\vikto\\python\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vikto\\python\\lib\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vikto\\python\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vikto\\python\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vikto\\python\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.2-cp311-cp311-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/7.6 MB 21.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 23.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.8/7.6 MB 26.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.6/7.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 28.7 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl (187 kB)\n",
      "   ---------------------------------------- 0.0/187.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 187.6/187.6 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.47.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.2/2.2 MB 46.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.1/103.1 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.47.0 kiwisolver-1.4.5 matplotlib-3.8.2 pyparsing-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-image\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b078a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/USC/Rx.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b441ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Number of samples in the training dataset: 15211\n",
      "Number of samples in the test dataset: 3803\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from dataloader.loader_USC import PMnet_usc\n",
    "\n",
    "usc_training_data=PMnet_usc('data/USC/train.csv', 'data/USC')\n",
    "print(f\"Number of samples in the training dataset: {len(usc_training_data)}\")\n",
    "\n",
    "usc_test_data=PMnet_usc('data/USC/test.csv', 'data/USC')\n",
    "print(f\"Number of samples in the test dataset: {len(usc_test_data)}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "usc_training_dataloader = DataLoader(usc_training_data, batch_size=16, shuffle=True)\n",
    "usc_test_dataloader = DataLoader(usc_test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7afb78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: torch.Size([2, 256, 256]), Sample label: torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([16, 2, 256, 256])\n",
      "Labels batch shape: torch.Size([16, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "image, label = usc_training_data[0]  # Get the first sample\n",
    "print(f\"Sample image shape: {image.shape}, Sample label: {label.shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_4_show = image[0,:,:].unsqueeze(0).permute(1,2,0)\n",
    "\n",
    "plt.imshow(img_4_show.squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "train_features, train_labels = next(iter(usc_training_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbd3dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMNet(\n",
      "  (layer1): _Stem(\n",
      "    (conv1): _ConvBnReLU(\n",
      "      (conv): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (layer2): _ResLayer(\n",
      "    (block1): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block2): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (block3): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): _ResLayer(\n",
      "    (block1): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block2): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (block3): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): _ResLayer(\n",
      "    (block1): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block2): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (block3): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer5): _ResLayer(\n",
      "    (block1): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): _ConvBnReLU(\n",
      "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block2): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (block3): _Bottleneck(\n",
      "      (reduce): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (conv3x3): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (increase): _ConvBnReLU(\n",
      "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (aspp): _ASPP(\n",
      "    (stages): Module(\n",
      "      (c0): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (c1): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (c2): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (c3): _ConvBnReLU(\n",
      "        (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (imagepool): _ImagePool(\n",
      "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (conv): _ConvBnReLU(\n",
      "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): _ConvBnReLU(\n",
      "    (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (reduce): _ConvBnReLU(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.0010000000000000009, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (conv_up5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up4): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up3): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up2): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up1): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up0): Sequential(\n",
      "    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_up00): Sequential(\n",
      "    (0): Conv2d(130, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from network.PMNet import PMNet\n",
    "network = PMNet([3,3,3,3], [1,1,1], None, 16)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9739b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9cd47d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval import ROI_RMSE_Loss\n",
    "\n",
    "loss_fn = ROI_RMSE_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fe1a4ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [0/15211 (0%)] Loss: 0.517527\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [16/15211 (0%)] Loss: 0.416127\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [32/15211 (0%)] Loss: 0.387073\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [48/15211 (0%)] Loss: 0.391319\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [64/15211 (0%)] Loss: 0.396915\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [80/15211 (1%)] Loss: 0.386089\n",
      "ROI shape: {} torch.Size([16, 1, 256, 256])\n",
      "Target with ROI shape: {} torch.Size([16, 2, 256, 256])\n",
      "Train Epoch: 1 [96/15211 (1%)] Loss: 0.394084\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(usc_training_dataloader):\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m      8\u001b[0m     roi \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m0\u001b[39m, :, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROI shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, roi\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PMNetForEMC\\network\\PMNet.py:193\u001b[0m, in \u001b[0;36mPMNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    191\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x1)\n\u001b[0;32m    192\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(x2)\n\u001b[1;32m--> 193\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x4)\n\u001b[0;32m    195\u001b[0m x6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer5(x5)\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PMNetForEMC\\network\\PMNet.py:57\u001b[0m, in \u001b[0;36m_Bottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(x)\n\u001b[0;32m     56\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3x3(h)\n\u001b[1;32m---> 57\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrease\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(h)\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\Lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "for epoch in range(1,2):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(usc_training_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)    \n",
    "        roi = data[:, 0, :, :].unsqueeze(1)\n",
    "        target_with_roi = torch.cat((target, roi), dim=1)\n",
    "        loss = loss_fn(output,target_with_roi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(usc_training_dataloader.sampler),\n",
    "                    100.0 * batch_idx / len(usc_training_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a6509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
